{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07991927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onnx has to be imported before torch (some bug)\n",
    "import onnx \n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed8fc671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rstar900/finn/deps/qonnx/src/qonnx/core/modelwrapper.py:93: UserWarning: Some old-style domain attributes were automatically converted to new-style,\n",
      "                i.e. domain=finn to domain=qonnx.custom_op.<general|fpgadataflow|...>\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import model into FINN with ModelWrapper\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "\n",
    "ready_model_filename = \"finn_lenet.onnx\"\n",
    "model_for_sim = ModelWrapper(ready_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc243bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_model_proto',\n",
       " 'analysis',\n",
       " 'check_all_tensor_shapes_specified',\n",
       " 'check_compatibility',\n",
       " 'cleanup',\n",
       " 'find_consumer',\n",
       " 'find_consumers',\n",
       " 'find_direct_predecessors',\n",
       " 'find_direct_successors',\n",
       " 'find_producer',\n",
       " 'find_upstream',\n",
       " 'fix_float64',\n",
       " 'get_all_tensor_names',\n",
       " 'get_finn_nodes',\n",
       " 'get_initializer',\n",
       " 'get_metadata_prop',\n",
       " 'get_node_index',\n",
       " 'get_nodes_by_op_type',\n",
       " 'get_non_finn_nodes',\n",
       " 'get_tensor_datatype',\n",
       " 'get_tensor_fanout',\n",
       " 'get_tensor_layout',\n",
       " 'get_tensor_shape',\n",
       " 'get_tensor_sparsity',\n",
       " 'get_tensor_valueinfo',\n",
       " 'graph',\n",
       " 'is_fork_node',\n",
       " 'is_join_node',\n",
       " 'make_empty_exec_context',\n",
       " 'make_new_valueinfo_name',\n",
       " 'model',\n",
       " 'rename_tensor',\n",
       " 'save',\n",
       " 'set_initializer',\n",
       " 'set_metadata_prop',\n",
       " 'set_tensor_datatype',\n",
       " 'set_tensor_layout',\n",
       " 'set_tensor_shape',\n",
       " 'set_tensor_sparsity',\n",
       " 'temporary_fix_oldstyle_domain',\n",
       " 'transform']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the available member functions of ModelWrapper\n",
    "dir(model_for_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c64f3084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor name: inp.1\n",
      "Output tensor name: 90\n",
      "Input tensor shape: [1, 3, 32, 32]\n",
      "Output tensor shape: [1, 10]\n",
      "Input tensor datatype: FLOAT32\n",
      "Output tensor datatype: FLOAT32\n",
      "List of node operator types in the graph: \n",
      "['MultiThreshold', 'Add', 'Mul', 'Conv', 'Mul', 'Div', 'Add', 'Mul', 'MultiThreshold', 'Mul', 'MaxPool', 'Conv', 'Mul', 'Div', 'Add', 'Mul', 'MultiThreshold', 'Mul', 'MaxPool', 'Shape', 'Gather', 'Unsqueeze', 'Concat', 'Reshape', 'MatMul', 'Mul', 'Div', 'Add', 'Mul', 'MultiThreshold', 'Mul', 'MatMul', 'Mul', 'Div', 'Add', 'Mul', 'MultiThreshold', 'Mul', 'MatMul', 'Mul']\n"
     ]
    }
   ],
   "source": [
    "# Extract various information using some of these functions \n",
    "# Note that the output tensor is (as of yet) marked as a float32 value, even though we know the output is binary \n",
    "# This will be automatically inferred by the compiler in the next step when we run the InferDataTypes transformation\n",
    "from qonnx.core.datatype import DataType\n",
    "\n",
    "finnonnx_in_tensor_name = model_for_sim.graph.input[0].name\n",
    "finnonnx_out_tensor_name = model_for_sim.graph.output[0].name\n",
    "print(\"Input tensor name: %s\" % finnonnx_in_tensor_name)\n",
    "print(\"Output tensor name: %s\" % finnonnx_out_tensor_name)\n",
    "finnonnx_model_in_shape = model_for_sim.get_tensor_shape(finnonnx_in_tensor_name)\n",
    "finnonnx_model_out_shape = model_for_sim.get_tensor_shape(finnonnx_out_tensor_name)\n",
    "print(\"Input tensor shape: %s\" % str(finnonnx_model_in_shape))\n",
    "print(\"Output tensor shape: %s\" % str(finnonnx_model_out_shape))\n",
    "finnonnx_model_in_dt = model_for_sim.get_tensor_datatype(finnonnx_in_tensor_name)\n",
    "finnonnx_model_out_dt = model_for_sim.get_tensor_datatype(finnonnx_out_tensor_name)\n",
    "print(\"Input tensor datatype: %s\" % str(finnonnx_model_in_dt.name))\n",
    "print(\"Output tensor datatype: %s\" % str(finnonnx_model_out_dt.name))\n",
    "print(\"List of node operator types in the graph: \")\n",
    "print([x.op_type for x in model_for_sim.graph.node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2dfd92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network preparation: Tidy-up transformations\n",
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "\n",
    "model_for_sim = model_for_sim.transform(InferShapes())\n",
    "model_for_sim = model_for_sim.transform(FoldConstants())\n",
    "model_for_sim = model_for_sim.transform(GiveUniqueNodeNames())\n",
    "model_for_sim = model_for_sim.transform(GiveReadableTensorNames())\n",
    "model_for_sim = model_for_sim.transform(InferDataTypes())\n",
    "model_for_sim = model_for_sim.transform(RemoveStaticGraphInputs())\n",
    "\n",
    "verif_model_filename = \"finn_lenet_verification.onnx\"\n",
    "model_for_sim.save(verif_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62622210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'finn_lenet_verification.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f9570ebea00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize in Netron\n",
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "showInNetron(verif_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b8f1f03",
   "metadata": {},
   "outputs": [],
   "source": [
    " # We will now do verification to make sure our transformed model performs exactly as the Brevitas one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87ecc785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Import for Brevitas\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.quant import Int8Bias as BiasQuant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29bfb843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# transform PILImage images from dataset to tensors\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
    "                                         shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, \n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a65ea3a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truck\n"
     ]
    }
   ],
   "source": [
    "# Just showing images to check if it works\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5 # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "# Get some random training image\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f99d456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we redefine the Brevitas low precision Lenet5 network\n",
    "class LowPrecisionNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LowPrecisionNet, self).__init__()\n",
    "        \n",
    "        # Input quantization layer\n",
    "        self.quant_inp = qnn.QuantIdentity(bit_width=4, return_quant_tensor=True)\n",
    "        \n",
    "        # 2 convolution layers and Quantized ReLU between each of them\n",
    "        # 3 input image channel, 6 output channels, 5x5 convolution kernel\n",
    "        self.conv1 = qnn.QuantConv2d(3, 6, 5, weight_bit_width=3, bias_quant=BiasQuant, return_quant_tensor=True)\n",
    "        self.relu1 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "        self.conv2 = qnn.QuantConv2d(6, 16, 5, weight_bit_width=3, bias_quant=BiasQuant, return_quant_tensor=True)\n",
    "        self.relu2 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "        \n",
    "        # 3 fully connected layers\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = qnn.QuantLinear(16 * 5 * 5, 120, bias=True, weight_bit_width=3, bias_quant=BiasQuant, return_quant_tensor=True)\n",
    "        self.relu3 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "        self.fc2 = qnn.QuantLinear(120, 84, bias=True, weight_bit_width=3, bias_quant=BiasQuant, return_quant_tensor=True)\n",
    "        self.relu4 = qnn.QuantReLU(bit_width=4, return_quant_tensor=True)\n",
    "        self.fc3 = qnn.QuantLinear(84, 10, bias=False, weight_bit_width=3)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.quant_inp(x)\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.reshape(x.shape[0], -1) # Flatten\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.relu4(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdf44be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load weights into the Brevitas model (Need to save the model as torch .pth as well beforehand)\n",
    "brevitas_model = LowPrecisionNet()\n",
    "trained_state_dict = torch.load(\"finn_lenet_state_dict.pth\")\n",
    "brevitas_model.load_state_dict(trained_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6299718d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 49 %\n"
     ]
    }
   ],
   "source": [
    "# Benchmarking Brevitas model over the whole dataset\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Not calculating gradients as we are not training\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = brevitas_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %') \n",
    "\n",
    "brevitas_model_accuracy = 100 * correct // total;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6537af38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 49 %\n"
     ]
    }
   ],
   "source": [
    "import finn.core.onnx_exec as oxe\n",
    "\n",
    "# Doing some transformations before data could be passed to the FINN model\n",
    "finnonnx_in_tensor_name = model_for_sim.graph.input[0].name\n",
    "finnonnx_model_in_shape = model_for_sim.get_tensor_shape(finnonnx_in_tensor_name)\n",
    "finnonnx_out_tensor_name = model_for_sim.graph.output[0].name\n",
    "\n",
    "# Now benchmarking the FINN's verification model over the same dataset\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Not calculating gradients as we are not training\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        \n",
    "        # Transforming input data before passing to FINN\n",
    "        images = images.detach().numpy()\n",
    "        \n",
    "        # reshape to expected input (add 1 for batch dimension)\n",
    "        images = images.reshape(finnonnx_model_in_shape)\n",
    "        \n",
    "        # create the input dictionary\n",
    "        input_dict = {finnonnx_in_tensor_name : images}\n",
    "        \n",
    "        # run with FINN's execute_onnx\n",
    "        output_dict = oxe.execute_onnx(model_for_sim, input_dict)\n",
    "        \n",
    "        #get the output tensor\n",
    "        outputs = output_dict[finnonnx_out_tensor_name]\n",
    "        \n",
    "        # convert to tensor (for evaluation)\n",
    "        outputs = torch.tensor(outputs)\n",
    "        \n",
    "        # benchmarking\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %') \n",
    "\n",
    "finn_model_accuracy = 100 * correct // total;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e232612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification Success!\n"
     ]
    }
   ],
   "source": [
    "# print verification results\n",
    "if brevitas_model_accuracy == finn_model_accuracy:\n",
    "    print(\"Verification Success!\")\n",
    "else:\n",
    "    print(\"Verification failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5288ae23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
